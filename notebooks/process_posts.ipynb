{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG5INxhj3NpV"
      },
      "outputs": [],
      "source": [
        "# Uncomment this cell if using gdrive and colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# root_folder = '/content/drive/MyDrive/<root folder with data from previous step>'\n",
        "# !pip install openai-whisper==20240930 opencv-python==4.11.0.86 paddleocr==2.10.0\n",
        "# !pip install paddlepaddle-gpu==3.0.0 -i https://www.paddlepaddle.org.cn/packages/stable/cu126/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7hHrwZd3rmv"
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements2.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIPhbQnIMwq9"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import pytesseract\n",
        "from paddleocr import PaddleOCR\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import paddle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCYB_xMTZjHk"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    root_folder\n",
        "except NameError:\n",
        "    root_folder = os.path.join(os.pardir, \"data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lopiJ1oqjZA4"
      },
      "outputs": [],
      "source": [
        "def is_similar_box(points1, points2, threshold=100):\n",
        "    difference = 0\n",
        "    for (x1, y1), (x2, y2) in zip(points1, points2):\n",
        "        difference += abs(x1 - x2) + abs(y1 - y2)\n",
        "    return difference < threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fvH27rO5NpQe"
      },
      "outputs": [],
      "source": [
        "def levenshtein_distance(s1, s2):\n",
        "    \"\"\"Calculate the Levenshtein distance between two strings.\"\"\"\n",
        "    if len(s1) < len(s2):\n",
        "        return levenshtein_distance(s2, s1)\n",
        "\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "\n",
        "    previous_row = range(len(s2) + 1)\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return previous_row[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c2Dj5QvyyQrS"
      },
      "outputs": [],
      "source": [
        "#filter to texts that appear for a few frames and fix minor OCR mistakes by majority vote rule\n",
        "def filter_texts(texts_by_frame, frame_dimensions, frames_threshold=4, error_threshold=2, levenshtein_distance_threshold=0.2):\n",
        "    filtered_texts = [] #[start frame, text, position]\n",
        "    text_positions = {} #\"text\": [frames_numbers, positions]\n",
        "\n",
        "    text_positions_heatmap = []\n",
        "\n",
        "    group_similar = {} #group_name: [text]\n",
        "\n",
        "    prev_frame_texts = {} #[text, key_name]\n",
        "\n",
        "    # generate heatmap of places where text appears\n",
        "    # filter to only text from most used area on the screen\n",
        "    for frame_texts in texts_by_frame:\n",
        "        if not frame_texts:\n",
        "            continue\n",
        "        for line in frame_texts:\n",
        "            box = line[0]\n",
        "            text = line[1][0].strip()\n",
        "\n",
        "            x = sum(p[0] for p in box) / 4\n",
        "            y = sum(p[1] for p in box) / 4\n",
        "\n",
        "            text_positions_heatmap.append((x, y))\n",
        "\n",
        "    xs, ys = zip(*text_positions_heatmap)\n",
        "\n",
        "    heatmap, yedges, xedges = np.histogram2d(np.copy(ys), np.copy(xs), bins=(5, 5))\n",
        "\n",
        "    plt.imshow(heatmap, cmap=\"hot\", interpolation='nearest')\n",
        "    plt.title(\"Text occurrence heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "    max_idx = np.unravel_index(np.argmax(heatmap), heatmap.shape)\n",
        "    y_bin, x_bin = max_idx\n",
        "\n",
        "    for frame_index, frame_texts in enumerate(texts_by_frame):\n",
        "        if True:\n",
        "            if not frame_texts:\n",
        "                continue\n",
        "            new_frame_texts = {}\n",
        "\n",
        "            for line in frame_texts:\n",
        "                box = line[0]\n",
        "                text = line[1][0].strip()\n",
        "\n",
        "                if not text:\n",
        "                    continue\n",
        "\n",
        "                x = sum(p[0] for p in box) / 4\n",
        "                y = sum(p[1] for p in box) / 4\n",
        "\n",
        "                #if text appears not in the most common region on the heatmap, skip it\n",
        "                if not (xedges[x_bin] <= x <= xedges[x_bin+1] and yedges[y_bin] <= y <= yedges[y_bin+1]):\n",
        "                    continue\n",
        "\n",
        "                # check levenstein distance to the previous words to minimize OCR error\n",
        "                if text in text_positions:\n",
        "                    text_positions[text][0].append(frame_index)\n",
        "                    text_positions[text][1].append(box)\n",
        "                    if text in prev_frame_texts:\n",
        "                        new_frame_texts[text] = prev_frame_texts.pop(text)\n",
        "                else:\n",
        "                    text_positions[text] = [[frame_index], [box]]\n",
        "                    found = False\n",
        "                    for prev_text, key_name in prev_frame_texts.items():\n",
        "                        #instead of checking all possible words, check similiarity with words from the last frame only\n",
        "                        if levenshtein_distance(text, prev_text)/len(text) < levenshtein_distance_threshold:\n",
        "                            group_similar[key_name].append(text)\n",
        "                            new_frame_texts[text] = key_name\n",
        "                            found = True\n",
        "                            break\n",
        "                    if not found:\n",
        "                        group_similar[text] = [text]\n",
        "                        new_frame_texts[text] = text\n",
        "\n",
        "\n",
        "            prev_frame_texts = new_frame_texts\n",
        "\n",
        "\n",
        "    #find groups with similar levenstein distance and merge them\n",
        "    group_similar_new = group_similar.copy()\n",
        "    for key1, similar_texts in group_similar.items():\n",
        "        if key1 not in group_similar_new:\n",
        "            continue\n",
        "        start = False\n",
        "        for key2, similar_texts2 in group_similar.items():\n",
        "            if key1 == key2:\n",
        "                start = True\n",
        "                continue\n",
        "            if start:\n",
        "                if levenshtein_distance(key1, key2)/len(key1) < levenshtein_distance_threshold:\n",
        "                    try:\n",
        "                        group_similar_new[key1].extend(group_similar_new.pop(key2))\n",
        "                    except KeyError:\n",
        "                        pass\n",
        "\n",
        "\n",
        "    #choose the most popular word in a group and update text positions\n",
        "    group_similar = group_similar_new\n",
        "    for _, similar_texts in group_similar.items():\n",
        "        most_used_text = max(similar_texts, key=lambda x: len(text_positions[x][0]))\n",
        "        for text in similar_texts:\n",
        "            if text != most_used_text:\n",
        "                text_positions[most_used_text][0].extend(text_positions[text][0])\n",
        "                text_positions[most_used_text][1].extend(text_positions.pop(text)[1])\n",
        "\n",
        "    #remove text that wasn't staying long in the same position\n",
        "    #captions on the video are usually not moving\n",
        "    for text, (frames_numbers, positions) in text_positions.items():\n",
        "        current_list = []\n",
        "        frames_and_positions = list(zip(frames_numbers, positions))\n",
        "        frames_and_positions.sort(key=lambda x: x[0])\n",
        "        for i in range(1, len(frames_and_positions)):\n",
        "            if frames_and_positions[i][0] - frames_and_positions[i-1][0] >= error_threshold or not is_similar_box(frames_and_positions[i-1][1], frames_and_positions[i][1]):\n",
        "                if len(current_list) >= frames_threshold:\n",
        "                    filtered_texts.append([current_list[0], text, frames_and_positions[i-1][1]])\n",
        "                current_list = []\n",
        "            else:\n",
        "                current_list.append(frames_and_positions[i-1][0])\n",
        "        if len(current_list) >= frames_threshold:\n",
        "            filtered_texts.append([current_list[0], text, positions[i-1]])\n",
        "    #sort text by frames\n",
        "    filtered_texts.sort(key=lambda x: x[0])\n",
        "    return filtered_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7DM0Njqo2E3w"
      },
      "outputs": [],
      "source": [
        "def group_lines_by_frame(results, tolerance = 3): #[frame, text, position]\n",
        "    results.sort(key=lambda x: x[0])\n",
        "\n",
        "    groups = []\n",
        "    current_group = [results[0]]\n",
        "\n",
        "    for i in range(1, len(results)):\n",
        "        if results[i][0] - results[i-1][0] <= tolerance:\n",
        "            current_group.append(results[i])\n",
        "        else:\n",
        "            groups.append(current_group)\n",
        "            current_group = [results[i]]\n",
        "\n",
        "    groups.append(current_group)\n",
        "\n",
        "    return groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mYlVkgnd4HD7"
      },
      "outputs": [],
      "source": [
        "# combine text lines to create sentences in the correct order\n",
        "def merge_lines(lines, y_threshold=30, frame_tolerance = 3):\n",
        "    merged = \"\"\n",
        "    last_pos = -100\n",
        "\n",
        "    for group in group_lines_by_frame(lines, frame_tolerance):\n",
        "        group.sort(key=lambda x: x[2][0][1]) # sort by y position from top to bottom\n",
        "        for _, text, position in group:\n",
        "            merged += \" \" + text\n",
        "            last_pos = position[3][1]\n",
        "        last_pos = group[-1][2][0][1]\n",
        "\n",
        "    return merged\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RFst9bemwS7d"
      },
      "outputs": [],
      "source": [
        "# function to process image only posts\n",
        "def process_img_post(image_list,ocr):\n",
        "    MIN_CONFIDENCE = 0.7\n",
        "    ocr_results = \"\"\n",
        "\n",
        "    for img_path in image_list:\n",
        "        image = cv2.imread(img_path)\n",
        "        results = ocr.ocr(image, cls=True)\n",
        "\n",
        "        if not results[0]:\n",
        "            continue\n",
        "\n",
        "        for i in results[0]:\n",
        "            if i[1][1] >= MIN_CONFIDENCE:\n",
        "                ocr_results += i[1][0] + \" \"\n",
        "\n",
        "        ocr_results += \"\\n\"\n",
        "\n",
        "    return ocr_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRNplPS9t1-v"
      },
      "outputs": [],
      "source": [
        "def process_post(post_id,root_folder,ocr):\n",
        "    mp4_files = glob.glob(os.path.join(root_folder, post_id, '*.mp4'))\n",
        "    #if multiple images/videos extract data only from images\n",
        "    if not mp4_files or len(mp4_files) > 1:\n",
        "        print(f\"{post_id} - photos post\")\n",
        "        img_files = glob.glob(os.path.join(root_folder, post_id, '*.jpg'))\n",
        "        with open(os.path.join(root_folder, post_id, \"extracted_text.txt\"), 'w', encoding='utf-8') as f:\n",
        "            f.write(process_img_post(img_files,ocr))\n",
        "    else:\n",
        "        # if video extract data from audio\n",
        "        print(f\"{post_id} - video post\")\n",
        "        result = model.transcribe(mp4_files[0], language=\"en\")\n",
        "        if len(result[\"text\"]) > 100:\n",
        "            print(\"Audio text is more than 100 characters, skip screen text extraction\")\n",
        "            with open(os.path.join(root_folder, post_id, \"extracted_text.txt\"), 'w', encoding='utf-8') as f:\n",
        "                f.write(result[\"text\"])\n",
        "        else:\n",
        "            #if audio is too short look for text on the framse\n",
        "            print(\"Audio text is less than 100 characters, start screen text extraction\")\n",
        "\n",
        "            MIN_CONFIDENCE = 0.7\n",
        "\n",
        "            cap = cv2.VideoCapture(mp4_files[0])\n",
        "\n",
        "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "            frame_rate = 0.2  # frames per second to analyze\n",
        "            frame_idx = 0\n",
        "            frame_dimensions = (0,0)\n",
        "\n",
        "            ocr_results = []\n",
        "\n",
        "            while cap.isOpened():\n",
        "                ret, frame = cap.read()\n",
        "                if frame_dimensions[0] == 0 or frame_dimensions[1] == 0:\n",
        "                    frame_dimensions = frame.shape[:2]\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                if frame_idx % int(fps * frame_rate) == 0:\n",
        "                    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                    # Run OCR\n",
        "                    results = ocr.ocr(img_rgb, cls=True)\n",
        "                    if not results[0]:\n",
        "                        continue\n",
        "                    ocr_results.append([i for i in results[0] if i[1][1] >= MIN_CONFIDENCE])\n",
        "                frame_idx += 1\n",
        "\n",
        "            cap.release()\n",
        "\n",
        "            extracted_text = merge_lines(filter_texts(ocr_results, frame_dimensions, error_threshold=5, frames_threshold=3))\n",
        "            with open(os.path.join(root_folder, post_id, \"extracted_text.txt\"), 'w', encoding='utf-8') as f:\n",
        "                f.write(extracted_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7Wzs6spwQgn"
      },
      "outputs": [],
      "source": [
        "ocr = PaddleOCR(use_angle_cls=False, lang='en', use_gpu=True, show_log=False)\n",
        "logging.getLogger('ppocr').setLevel(logging.ERROR)\n",
        "model = whisper.load_model(\"medium\")\n",
        "counter = 1\n",
        "\n",
        "for post_id in os.listdir(root_folder):\n",
        "    print(f\"{counter}/{len(os.listdir(root_folder))}\")\n",
        "    counter += 1\n",
        "    try:\n",
        "        process_post(post_id,root_folder, ocr)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing post {post_id}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NDe6Oiwhea5m"
      },
      "outputs": [],
      "source": [
        "#combine all information into one txt file\n",
        "for post_id in os.listdir(root_folder):\n",
        "    try:\n",
        "        txt_files = glob.glob(os.path.join(root_folder, post_id, '*.txt'))\n",
        "        txt_files.sort()\n",
        "        #can be changed json if needed\n",
        "        with open(os.path.join(root_folder, post_id, \"extracted_text_final.txt\"), 'w', encoding='utf-8') as fw, open(txt_files[0], 'r', encoding='utf-8') as post_descr, open(txt_files[1], 'r', encoding='utf-8') as post_extracted:\n",
        "            fw.write(f\"Instagram post {post_id}:\")\n",
        "            fw.write(\"\\n\\nPost description:\\n\")\n",
        "            fw.write(post_descr.read())\n",
        "            fw.write(\"\\n\\nPost extracted text:\\n\")\n",
        "            fw.write(post_extracted.read())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found for post {post_id}\")\n",
        "        continue"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
